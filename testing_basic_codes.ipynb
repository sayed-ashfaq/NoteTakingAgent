{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fc0533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey1\n"
     ]
    }
   ],
   "source": [
    "print(\"Hey1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a149dc64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='hi!', additional_kwargs={}, response_metadata={}, id='0ac7d024-b9b1-47b5-a2cf-227b32b5a02b'),\n",
       "  AIMessage(content='hello world', additional_kwargs={}, response_metadata={}, id='5b36477a-2357-465c-9e30-c5d231a1726c', tool_calls=[], invalid_tool_calls=[])]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## testint langgraph\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "\n",
    "def mock_llm(state: MessagesState):\n",
    "    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\n",
    "\n",
    "graph = StateGraph(MessagesState)\n",
    "graph.add_node(mock_llm)\n",
    "graph.add_edge(START, \"mock_llm\")\n",
    "graph.add_edge(\"mock_llm\", END)\n",
    "graph = graph.compile()\n",
    "\n",
    "graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9026e99",
   "metadata": {},
   "source": [
    "Current Goal: \n",
    "1. Build an agent that can take my input text\n",
    "2. Identify wheather it is Idea, task to perform, notes\n",
    "3. Slightly format them and convert them to Markdown\n",
    "4. Add tags to it - Try to use a machine learning - NLP\n",
    "5. Add that data to the notion in designation pages such as Ideas, Daily Tasks- Create page,\n",
    "6. Create page for notes and add them to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a3eacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "from pathlib import Path \n",
    "\n",
    "sys.path.append(str(Path().resolve().parents[1])) # This doesn't work in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a66ea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "sys.path.insert(0, os.path.abspath(\"..\")) # => This works in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5337c5e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unsupported provider='google'.\n\nSupported model providers are: anthropic, azure_ai, azure_openai, bedrock, bedrock_converse, cohere, deepseek, fireworks, google_anthropic_vertex, google_genai, google_vertexai, groq, huggingface, ibm, mistralai, nvidia, ollama, openai, perplexity, together, upstage, xai",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrouter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_llm\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m llm = \u001b[43mget_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m llm\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\Generative AI\\VA_NoteTaking\\src\\llm\\router.py:9\u001b[39m, in \u001b[36mget_llm\u001b[39m\u001b[34m(role)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m role \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _models:\n\u001b[32m      8\u001b[39m     cfg = settings.data[\u001b[33m\"\u001b[39m\u001b[33mllm\u001b[39m\u001b[33m\"\u001b[39m][role]\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     _models[role] = \u001b[43minit_chat_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprovider\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _models[role]\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\Generative AI\\VA_NoteTaking\\.venv\\Lib\\site-packages\\langchain\\chat_models\\base.py:464\u001b[39m, in \u001b[36minit_chat_model\u001b[39m\u001b[34m(model, model_provider, configurable_fields, config_prefix, **kwargs)\u001b[39m\n\u001b[32m    456\u001b[39m     warnings.warn(\n\u001b[32m    457\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_prefix\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m has been set but no fields are configurable. Set \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    458\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`configurable_fields=(...)` to specify the model params that are \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    459\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mconfigurable.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    460\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    461\u001b[39m     )\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m configurable_fields:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_init_chat_model_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model:\n\u001b[32m    470\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m] = model\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\Generative AI\\VA_NoteTaking\\.venv\\Lib\\site-packages\\langchain\\chat_models\\base.py:487\u001b[39m, in \u001b[36m_init_chat_model_helper\u001b[39m\u001b[34m(model, model_provider, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_init_chat_model_helper\u001b[39m(\n\u001b[32m    481\u001b[39m     model: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    482\u001b[39m     *,\n\u001b[32m    483\u001b[39m     model_provider: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    484\u001b[39m     **kwargs: Any,\n\u001b[32m    485\u001b[39m ) -> BaseChatModel:\n\u001b[32m    486\u001b[39m     model, model_provider = _parse_model(model, model_provider)\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m     creator_func = \u001b[43m_get_chat_model_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_provider\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m creator_func(model=model, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\Generative AI\\VA_NoteTaking\\.venv\\Lib\\site-packages\\langchain\\chat_models\\base.py:150\u001b[39m, in \u001b[36m_get_chat_model_creator\u001b[39m\u001b[34m(provider)\u001b[39m\n\u001b[32m    148\u001b[39m     supported = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(_BUILTIN_PROVIDERS.keys())\n\u001b[32m    149\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprovider\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSupported model providers are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    152\u001b[39m pkg, class_name, creator_func = _BUILTIN_PROVIDERS[provider]\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: Unsupported provider='google'.\n\nSupported model providers are: anthropic, azure_ai, azure_openai, bedrock, bedrock_converse, cohere, deepseek, fireworks, google_anthropic_vertex, google_genai, google_vertexai, groq, huggingface, ibm, mistralai, nvidia, ollama, openai, perplexity, together, upstage, xai"
     ]
    }
   ],
   "source": [
    "from src.llm.router import get_llm\n",
    "\n",
    "llm = get_llm(\"reasoning\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300c6e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os \n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
